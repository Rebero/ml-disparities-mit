{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LCP_Pull_Extract.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvZuugqstzbUoVa2iYK7HG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atjoelpark/ml-disparities-mit/blob/master/pull_preprocessing/LCP_Pull_Extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ou01sFNHqII"
      },
      "source": [
        "# Pull and Extraction of LCP PMIDs\n",
        "\n",
        "This notebook provides documentation and code for pulling metadata for a list of PMID IDs.\n",
        "\n",
        "Reference: https://lcp.mit.edu/publications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5F68NWGnNJT"
      },
      "source": [
        "# Customization\n",
        "# Please enter in the pathway in your Google Drive (after /content/drive/) that you would like your files to be saved into\n",
        "# Users will only need to modify this code then run all the cells in order\n",
        "google_drive_url = \"\"\n",
        "folder_to_save_in_google_drive = \"/content/drive/My Drive/Research/Pubmed/MIT_publications\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i7Z0eLsnKp3"
      },
      "source": [
        "# Libraries and Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9gwfQKjHHOF"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re \n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2CMbEmxcI52",
        "outputId": "1de019b3-1462-4845-fd29-4bb8b0ed5ec0"
      },
      "source": [
        "# Mounting Google Drive if using Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(f'/content/drive/{google_drive_url}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHftg_fXkRLd"
      },
      "source": [
        "## Defining Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI4WyW-wlVY0"
      },
      "source": [
        "# Installing E-utilities Entrez Direct\n",
        "def e_utilities_install():\n",
        "  \"\"\"\n",
        "  Installs e_utilities\n",
        "  Reference: https://www.ncbi.nlm.nih.gov/books/NBK179288/\n",
        "  \"\"\"\n",
        "  !sh -c \"$(curl -fsSL ftp://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/install-edirect.sh)\"\n",
        "  !echo 'export PATH=\\$PATH:\\$HOME/edirect' >> $HOME/.bash_profile"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ0eaIJLcmFa"
      },
      "source": [
        "# Setting development environment for Selenium\n",
        "def setup_dev_environment():\n",
        "  \"\"\"\n",
        "  Installs chromium, driver and selenium\n",
        "  Sets options to be headless\n",
        "  Opens a website and prepares Selenium for use\n",
        "  Returns: webdriver\n",
        "  \"\"\"\n",
        "\n",
        "  # install chromium, its driver, and selenium\n",
        "  !apt update\n",
        "  !apt install chromium-chromedriver\n",
        "  !pip install selenium\n",
        "  !pip install webdriver_manager\n",
        "  # set options to be headless, ..\n",
        "  from selenium import webdriver\n",
        "  options = webdriver.ChromeOptions()\n",
        "  options.add_argument('--headless')\n",
        "  options.add_argument('--no-sandbox')\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "  # open it, go to a website, and get results\n",
        "  print(\"Chromium, Driver and Selenium successfully started....\")\n",
        "  return webdriver.Chrome(options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG8QXqbkYL8G"
      },
      "source": [
        "# Defining Functions\n",
        "def pull_pmid(year: int, wd) -> list:\n",
        "  \"\"\" \n",
        "  This takes in a year as an argument and reads the PMID IDs for each year within https://lcp.mit.edu/publications\n",
        "  Please note that scraping is a fragile process. Any changes to the underlying HTML tags, attributes can break this process.\n",
        "  This method was constructed so that extracting PMIDs would be an automated, reproducible process.\n",
        "  Note that manually inspecting the webpages and writing down the PMIDs is likely a faster and more reliable process!\n",
        "  Regardless, please use this method at your caution as the results may not be reliable as time continues forward.\n",
        "\n",
        "  Dependencies: Selenium, chromium-chromedriver, webdriver_manager, re\n",
        "  @param year: This int contains the year to scrape from in https://lcp.mit.edu/publications\n",
        "  @param wb: Passes in the webdriver for Selenium\n",
        "  @return: a list of PMIDs\n",
        "  @raise TypeError: raises an exception\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Uses Selenium to search by CSS\n",
        "    URL = f'https://lcp.mit.edu/publications#P_{year}'\n",
        "    wd.get(URL)\n",
        "    # Click on the appropriate year tag\n",
        "    l = wd.find_element_by_link_text(str(year))\n",
        "    l.click()\n",
        "\n",
        "    # Though this will decrease this method's generalizability, it appears that the CSS selector differs from year to year\n",
        "    # For now, will hard code this in from year 2003 to 2020 (inclusive)\n",
        "    year_switcher = {\n",
        "        2003: '.bib2xhtml a:nth-child(2)~ a+ a , dd:nth-child(4) cite+ a',\n",
        "        2004: '.bib2xhtml:nth-child(5) a:nth-child(4)',\n",
        "        2005: 'dd:nth-child(6) cite~ a+ a , .bib2xhtml a~ a+ a',\n",
        "        2006: '.bib2xhtml a:nth-child(2)~ a+ a , .bib2xhtml:nth-child(5) dd:nth-child(6) a+ a',\n",
        "        2007: 'dd:nth-child(6) cite~ a+ a , dd:nth-child(2) cite~ a+ a',\n",
        "        2008: '.bib2xhtml:nth-child(5) dd:nth-child(4) a+ a , .bib2xhtml:nth-child(5) dd:nth-child(2) a+ a , dd:nth-child(8) a+ a , dd:nth-child(14) a+ a , #content a:nth-child(5) , .bib2xhtml a:nth-child(2) , .bib2xhtml:nth-child(5) dd:nth-child(10) a+ a',\n",
        "        2009: '.bib2xhtml a+ a:nth-child(3) , dd:nth-child(6) a+ a , #content a:nth-child(5) , dd:nth-child(2) a+ a',\n",
        "        2010: 'dd:nth-child(16) a+ a , .bib2xhtml a:nth-child(2)~ a+ a , #content a:nth-child(5)',\n",
        "        2011: '.bib2xhtml a:nth-child(2)~ a+ a , .bib2xhtml:nth-child(5) dd:nth-child(6) a , #content a:nth-child(5) , dd:nth-child(2) a~ cite+ a',\n",
        "        2012: 'dd:nth-child(16) a+ a , #content a:nth-child(5) , dd:nth-child(2) a+ a',\n",
        "        2013: 'dd:nth-child(24) a+ a , dd:nth-child(22) a+ a , dd:nth-child(20) a , dd:nth-child(16) a+ a , dd:nth-child(14) a+ a , dd:nth-child(10) a+ a , dd:nth-child(12) a+ a , dd:nth-child(6) cite:nth-child(2)~ a+ a , #content a:nth-child(5)',\n",
        "        2014: 'dd:nth-child(28) a+ a , dd:nth-child(20) a+ a , .bib2xhtml a:nth-child(2)~ a+ a , .bib2xhtml:nth-child(5) dd:nth-child(6) a+ a , dd:nth-child(4) a+ a , #content a:nth-child(5)',\n",
        "        2015: 'dd:nth-child(4) a+ a , dd:nth-child(40) a+ a , dd:nth-child(38) a+ a , dd:nth-child(36) a+ a , dd:nth-child(32) a+ a , .bib2xhtml a:nth-child(2)~ a+ a , dd:nth-child(26) cite+ a , dd:nth-child(22) a+ a , dd:nth-child(14) a+ a , dd:nth-child(12) a+ a , .bib2xhtml:nth-child(5) dd:nth-child(6) a+ a , #content a:nth-child(5) , dd:nth-child(2) a+ a',\n",
        "        2016: 'dd:nth-child(6) a+ a , dd:nth-child(8) cite~ a+ a , dd:nth-child(54) a+ a , dd:nth-child(52) a+ a , dd:nth-child(50) a+ a , dd:nth-child(44) a+ a , dd:nth-child(46) a+ a , dd:nth-child(40) a~ a+ a , dd:nth-child(38) a+ a , dd:nth-child(34) a+ a , dd:nth-child(32) cite+ a , dd:nth-child(28) a+ a , dd:nth-child(20) cite~ a+ a , dd:nth-child(16) cite~ a+ a , dd:nth-child(4) cite~ a+ a , #content a:nth-child(5)',\n",
        "        2017: 'dd:nth-child(48) a+ a , dd:nth-child(46) a+ a , dd:nth-child(44) a+ a , dd:nth-child(40) a+ a , dd:nth-child(38) a+ a , dd:nth-child(36) a+ a , dd:nth-child(34) a+ a , dd:nth-child(32) a+ a , dd:nth-child(30) a+ a , dd:nth-child(26) a+ a , dd:nth-child(24) cite+ a , dd:nth-child(22) a+ a , dd:nth-child(20) a+ a , dd:nth-child(18) a+ a , dd:nth-child(16) a+ a , dd:nth-child(12) a+ a , dd:nth-child(10) a+ a , dd:nth-child(4) a+ a , #content a:nth-child(5) , dd:nth-child(2) a+ a',\n",
        "        2018: '.bib2xhtml:nth-child(5) a+ a',\n",
        "        2019: '.bib2xhtml a~ a+ a , cite~ a+ a',\n",
        "        2020: '.bib2xhtml a+ a'\n",
        "    }\n",
        "\n",
        "    # Python switch statement depending on year\n",
        "    def switch(year):\n",
        "      return year_switcher.get(year)\n",
        "\n",
        "    # Gathers PMIDS\n",
        "    _links = wd.find_elements_by_css_selector(switch(year))\n",
        "\n",
        "    # Initiating an empty PMID list and appends to list with all PMID IDs\n",
        "    _pmid_list = []\n",
        "    for i in _links:\n",
        "      tmp_search = re.findall(r'\\(PMID:.*\\)', i.text)\n",
        "      if tmp_search:\n",
        "        _pmid_list.append(tmp_search)\n",
        "\n",
        "    # Flattends the list\n",
        "    _pmid_list = [item for sublist in _pmid_list for item in sublist]\n",
        "\n",
        "    # Extracts out only integers and removes text and special characters. \n",
        "    # Returns the list\n",
        "    _pmid_list = [int(re.findall(r'\\d+', i)[0]) for i in _pmid_list]\n",
        "    return _pmid_list\n",
        "\n",
        "  except TypeError as e:\n",
        "    print(\"Error raised while pulling PMID...\")\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXr5QGofH8XC"
      },
      "source": [
        "# Defining Functions\n",
        "def pull_pmid_metadata(pmid: list) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  This is dependent on E-utilities. The input parameter is a list of PMIDs.\n",
        "  The output is is a Pandas DataFrame with the following columns:\n",
        "\n",
        "  1. PMID\n",
        "  2. PubMed Article Title\n",
        "  3. DateCompleted (Year_Month_Day)\n",
        "  4. DateRevised (Year_Month_Day)\n",
        "  5. Journal Title\n",
        "  6. Publication Date (Year_Month_Day)\n",
        "  7. Abstract\n",
        "  8. Author FirstName_LastName_Affiliation (Note that that three values are\n",
        "  separated by \"_\". If an author has affilitations to multiple institions, the\n",
        "  institutions are separated by the character \"/\".)\n",
        "\n",
        "  @param pmid: Takes a list of PMIDs produced by function pull_pmid\n",
        "  @return: Returns a Pandas DataFrame\n",
        "  @raise keyError: raises an exception\n",
        "  \"\"\"\n",
        "  columns = [\"PMID\", \"PubMed_Article_Title\", \"Date_Completed_Year\", \n",
        "             \"Date_Completed_Month\", \"Date_Completed_Day\", \"Date_Revised_Year\", \n",
        "             \"Date_Revised_Month\", \"Date_Revised_Day\", \"Journal_Title\",\n",
        "             \"Publication_Date_Year\", \"Publication_Date_Month\", \"Publication_Date_Day\",\n",
        "             \"Abstract\", \"AuthorFirstName_AuthorLastName_Affiliation\"]\n",
        "  df = pd.DataFrame(columns=columns)\n",
        "\n",
        "  for id, i in enumerate(pmid):\n",
        "    try:\n",
        "      _temp = f'''$HOME/edirect/efetch -db pubmed -id {i} -format xml \\\n",
        "| $HOME/edirect/xtract -pattern PubmedArticle -tab \"|\" -def \"NULL\" -sep \",\" -element MedlineCitation/PMID ArticleTitle \\\n",
        "DateCompleted/Year DateCompleted/Month DateCompleted/Day DateRevised/Year DateRevised/Month DateRevised/Day Journal/Title \\\n",
        "PubDate/Year PubDate/Month PubDate/Day AbstractText \\\n",
        "-block Author -tab \"/\" -def \"NULL\" -sep \"_\" -element ForeName,LastName,Affiliation'''\n",
        "\n",
        "      _result = !{_temp}\n",
        "      _temp = _result[0].split(\"|\")\n",
        "\n",
        "      for _count, _value in enumerate(_temp):\n",
        "        df.loc[id,columns[_count]] = _value\n",
        "        \n",
        "    except Exception as e:)\n",
        "      print(f\"Error Raised when Querying Unix EDirect for PMID: {i}\")\n",
        "      print(e)\n",
        "\n",
        "  # Prior to returning df\n",
        "  # If any cells have empty values, convert to NULL\n",
        "  df = df.replace(r'', \"NULL\", regex=True)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLhETuQhmGb6"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxvNWj7AlMFa",
        "outputId": "1801f18a-d414-4e13-9ef6-66671730740d"
      },
      "source": [
        "%%time\n",
        "# Install E-utilities, when prompted \"Would you like to do that automatically now?\" Please select 'y'.\n",
        "e_utilities_install()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrez Direct has been successfully downloaded and installed.\n",
            "\n",
            "-e In order to complete the configuration process, please execute the following:\n",
            "\n",
            "  echo \"source ~/.bash_profile\" >> $HOME/.bashrc\n",
            "\n",
            "or manually edit the PATH variable assignment in your .bash_profile file.\n",
            "\n",
            "Would you like to do that automatically now? [y/N]\n",
            "y\n",
            "OK, done.\n",
            "\n",
            "To activate EDirect for this terminal session, please execute the following:\n",
            "\n",
            "export PATH=${PATH}:$HOME/edirect\n",
            "\n",
            "CPU times: user 71.2 ms, sys: 23.6 ms, total: 94.7 ms\n",
            "Wall time: 6.87 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fxofv-JiCK5",
        "outputId": "eb6143f2-3040-4e4c-fd37-0b0d07e92853"
      },
      "source": [
        "%%time \n",
        "# Sets up Web Driver\n",
        "wd = setup_dev_environment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 4s (3,181 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "107 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 107 not upgraded.\n",
            "Need to get 86.0 MB of archives.\n",
            "After this operation, 298 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 91.0.4472.101-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 91.0.4472.101-0ubuntu0.18.04.1 [76.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 91.0.4472.101-0ubuntu0.18.04.1 [3,937 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 91.0.4472.101-0ubuntu0.18.04.1 [4,837 kB]\n",
            "Fetched 86.0 MB in 4s (23.3 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_91.0.4472.101-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_91.0.4472.101-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (91.0.4472.101-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[K     |████████████████████████████████| 904 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-3.4.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from webdriver_manager) (2.23.0)\n",
            "Collecting configparser\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting crayons\n",
            "  Downloading crayons-0.4.0-py2.py3-none-any.whl (4.6 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->webdriver_manager) (2021.5.30)\n",
            "Installing collected packages: colorama, crayons, configparser, webdriver-manager\n",
            "Successfully installed colorama-0.4.4 configparser-5.0.2 crayons-0.4.0 webdriver-manager-3.4.2\n",
            "Chromium, Driver and Selenium successfully started....\n",
            "CPU times: user 518 ms, sys: 112 ms, total: 630 ms\n",
            "Wall time: 41.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYW9zGS9DpxZ",
        "outputId": "75744bb7-d490-4646-f9da-073d4e0239f3"
      },
      "source": [
        "%%time\n",
        "# Creating a dictionary with keys = years, values = list of PMIDs corresponding to the year\n",
        "MIT_publications = {}\n",
        "for year in range(2003, 2021, 1):\n",
        "  MIT_publications[year] = pull_pmid(year, wd)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.18 s, sys: 306 ms, total: 7.49 s\n",
            "Wall time: 1min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3-TBKPOEyTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1350832f-5be0-41f3-e2ad-5f8b21022156"
      },
      "source": [
        "%%time\n",
        "# Looping through the dictionary keys and returning pandas dataframes\n",
        "# Saving to Google Colab all the CSV files\n",
        "MIT_publications_df = {}\n",
        "for year in range(2003, 2021, 1):\n",
        "  MIT_publications_df[year] = pull_pmid_metadata(MIT_publications[year])\n",
        "  exec(f'MIT_publications_{year} = MIT_publications_df[{year}]')\n",
        "  exec(f'MIT_publications_{year}.to_csv(\"{folder_to_save_in_google_drive}/MIT_publications_{year}.csv\", index=False)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error Raised when Querying Unix EDirect for PMID: 28114047\n",
            "list index out of range\n",
            "Error Raised when Querying Unix EDirect for PMID: 28328711\n",
            "list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}