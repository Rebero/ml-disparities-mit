{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNYELb6cADae",
    "outputId": "045258be-83bf-47bc-a574-a972a2e3f1ad"
   },
   "outputs": [],
   "source": [
    "#!pip install simpletransformers\n",
    "#!pip install wandb\n",
    "#!pip install torch\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, classification_report, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from simpletransformers.classification import ClassificationModel\n",
    "#import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SH1EtnengEvl",
    "outputId": "27e0276e-a7bc-40e6-9714-396d02c6e496"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colab\n",
    "#raw_data_path = '/content/drive/My Drive/NLP/ML_xlnet/training_test.xlsx'\n",
    "#destination_folder = '/content/drive/My Drive/NLP/ML_xlnet/preprocessed'\n",
    "\n",
    "#Local\n",
    "raw_data_path = 'Data/wo_Meta/unprocessed/training_test.xlsx'\n",
    "destination_folder = 'Data/wo_Meta/preprocessed'\n",
    "\n",
    "# Read raw data\n",
    "df_raw = pd.read_excel(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U-nqvzvRv3Md"
   },
   "outputs": [],
   "source": [
    "# Prepare columns\n",
    "df_raw['label'] = (df_raw['label'] == 'YES').astype('int')\n",
    "df_raw['titletext'] = df_raw['title'] + \". \" + df_raw['text']\n",
    "\n",
    "# Drop rows with empty text\n",
    "df_raw.drop( df_raw[df_raw.text.str.len() < 5].index, inplace=True)\n",
    "df_raw = df_raw[df_raw['title'].notnull()]\n",
    "#df_raw = df_raw[df_raw['text'].fillna(\"\")]\n",
    "\n",
    "def trim_string(x):\n",
    "\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Trim text and titletext to first_n_words\n",
    "#first_n_words = 1000\n",
    "#df_raw['text'] = df_raw['text'].apply(trim_string)\n",
    "#df_raw['titletext'] = df_raw['titletext'].apply(trim_string) \n",
    "\n",
    "# Drop title and text\n",
    "df_raw.drop([\"title\", \"text\"], axis= 1, inplace=True)\n",
    "\n",
    "# Drop emtpy rows\n",
    "nan_value = float(\"NaN\")\n",
    "df_raw.replace(\"\", nan_value, inplace=True)\n",
    "df_raw.dropna(subset = [\"titletext\"], inplace=True)\n",
    "\n",
    "# Assign X and y\n",
    "X = df_raw[[\"titletext\" ,\"DOI\"]]\n",
    "y = df_raw[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zd2cseZVJbWO"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train_pre, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_pre, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#We will not need this later, this is a preliminary mix of metadata filtering with NLP test-set\n",
    "#X_test_filtered = X_test[~X_test['DOI'].isin(DOIs_for_prefiltering)]\n",
    "#ids_X_test_filtered = X_test_filtered.index.values.tolist()\n",
    "#y_test_filtered = y_test[y_test.index.isin(ids_X_test_filtered)]\n",
    "\n",
    "#X_test = X_test.drop(\"DOI\", axis=1)\n",
    "#X_test_filtered = X_test_filtered.drop(\"DOI\", axis=1)\n",
    "\n",
    "## train valid split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X_train_pre, y_train_pre):\n",
    "    X_train, X_valid = X_train_pre.iloc[train_index], X_train_pre.iloc[test_index]\n",
    "    y_train, y_valid = y_train_pre.iloc[train_index], y_train_pre.iloc[test_index]\n",
    "\n",
    "# Write preprocessed data\n",
    "X_train.to_csv(destination_folder + '/X_train.csv', index=False)\n",
    "y_train.to_csv(destination_folder + '/y_train.csv', index=False)\n",
    "X_test.to_csv(destination_folder + '/X_test.csv', index=False)\n",
    "y_test.to_csv(destination_folder + '/y_test.csv', index=False)\n",
    "X_valid.to_csv(destination_folder + '/X_valid.csv', index=False)\n",
    "y_valid.to_csv(destination_folder + '/y_valid.csv', index=False)\n",
    "\n",
    "X_test_filtered.to_csv(destination_folder + '/X_test_filtered.csv', index=False)\n",
    "y_test_filtered.to_csv(destination_folder + '/y_test_filtered.csv', index=False)\n",
    "\n",
    "df_train = X_train.to_frame().join(y_train)\n",
    "df_test = X_test.to_frame().join(y_test)\n",
    "df_valid = X_valid.to_frame().join(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlROfT-UJflA"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./X_train.csv\")\n",
    "y_train = pd.read_csv(\"./y_train.csv\")\n",
    "X_test = pd.read_csv(\"./X_test.csv\")\n",
    "y_test = pd.read_csv(\"./y_test.csv\")\n",
    "\n",
    "df_train = X_train\n",
    "df_train[\"label\"] = y_train\n",
    "df_test = X_test\n",
    "df_test[\"label\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxaztyGQam4Y"
   },
   "source": [
    "## Pre-Processing and matching metadata with annotations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x-meFAAXaqvS"
   },
   "outputs": [],
   "source": [
    "#Colab\n",
    "#raw_data_path_meta = '/content/drive/My Drive/NLP/Data/w_Meta/raw/Pubmed_final_df_v2.csv'\n",
    "#raw_data_path = '/content/drive/My Drive/NLP/Data/wo_Meta/raw/annotated/Scope extract (1).xlsx'\n",
    "#destination_folder = '/content/drive/My Drive/NLP/Data/w_Meta/preprocessed'\n",
    "\n",
    "#local\n",
    "raw_data_path_meta = 'Data/w_Meta/raw/Pubmed_final_df_v2.csv'\n",
    "raw_data_path = 'Data/wo_Meta/raw/annotated/Scope extract (1).xlsx'\n",
    "destination_folder = 'Data/w_Meta/preprocessed'\n",
    "\n",
    "# Read raw data\n",
    "df_raw_meta = pd.read_csv(raw_data_path_meta)\n",
    "df_raw = pd.read_excel(raw_data_path)\n",
    "df_raw = df_raw.rename(columns={'Title': 'ArticleTitle'})\n",
    "\n",
    "#doi2pmid = \"/content/drive/My Drive/NLP/Data/DOI2PMID/DOIs and PMIDs (1).csv\"\n",
    "#doi2pmid2 = \"/content/drive/My Drive/NLP/Data/DOI2PMID/DOIs and PMIDs (2).csv\"\n",
    "#text2pmid = \"/content/drive/My Drive/NLP/Data/wo_Meta/raw/Pubmed_final_df.csv\"\n",
    "\n",
    "#df_doi2pmid = pd.read_csv(doi2pmid)\n",
    "#df_doi2pmid2 = pd.read_csv(doi2pmid2)\n",
    "#df_text2pmid = pd.read_csv(text2pmid)\n",
    "\n",
    "#df_doi2pmid2.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "df_merged = pd.merge(df_raw_meta, df_raw, how='inner', on=['DOI'])\n",
    "\n",
    "df_merged.MESH = df_merged.MESH.str.replace(\"'\", \"\");\n",
    "df_merged.MESH = df_merged.MESH.str.replace(\"[\", \"\");\n",
    "df_merged.MESH = df_merged.MESH.str.replace(\"]\", \"\");\n",
    "df_merged.MESH = df_merged.MESH.str.replace(\" \", \"\");\n",
    "df_merged.MESH = df_merged.MESH.fillna(\"No data provided\")\n",
    "\n",
    "df_merged.Keywords = df_merged.Keywords.str.replace(\"'\", \"\");\n",
    "df_merged.Keywords = df_merged.Keywords.str.replace(\"[\", \"\");\n",
    "df_merged.Keywords = df_merged.Keywords.str.replace(\"]\", \"\");\n",
    "df_merged.Keywords = df_merged.Keywords.str.replace(\" \", \"\");\n",
    "df_merged.Keywords = df_merged.Keywords.fillna(\"\")\n",
    "\n",
    "df_merged = df_merged.drop([\"Unnamed: 0\", \"AbstractText\", \"ArticleTitle_y\"], axis=1)\n",
    "df_merged = df_merged.rename(columns={\"ArticleTitle_x\": \"ArticleTitle\"})\n",
    "\n",
    "# Colab\n",
    "#df_merged.to_csv('/content/drive/My Drive/NLP/Data/w_Meta/annotated/merged_1000only.csv', index=False)\n",
    "\n",
    "# Local\n",
    "df_merged.to_csv('./Data/w_Meta/annotated/merged_1000only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UP_CAiE_jDjI"
   },
   "outputs": [],
   "source": [
    "#Colab\n",
    "#data_merged = '/content/drive/My Drive/NLP/Data/w_Meta/annotated/merged_1000only.csv'\n",
    "\n",
    "#Local\n",
    "data_merged = 'Data/w_Meta/annotated/merged_1000only.csv'\n",
    "\n",
    "df_merged = pd.read_csv(data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(x):\n",
    "    try:\n",
    "        return x.split(',')\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "dic_MESH = df_merged.MESH.values\n",
    "dic_MESH = pd.Series(dic_MESH)\n",
    "dic_MESH = dic_MESH.apply(converter)\n",
    "dic_MESH = dic_MESH.to_numpy()\n",
    "\n",
    "dic_key = df_merged.Keywords.values\n",
    "dic_key = pd.Series(dic_key)\n",
    "dic_key = dic_key.apply(converter)\n",
    "dic_key = dic_key.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHTi0LPjNFl"
   },
   "source": [
    "## Make Dictionary / Frequeuncies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_7LKngAojQKi"
   },
   "outputs": [],
   "source": [
    "frequency_flattened = []\n",
    "for list in dic_MESH:\n",
    "  for item in list:\n",
    "    item = item.strip()\n",
    "    frequency_flattened.append(item)\n",
    "\n",
    "wordfreq = {}\n",
    "for word in frequency_flattened:\n",
    "    if word not in wordfreq:\n",
    "        wordfreq[word] = 0 \n",
    "    wordfreq[word] += 1\n",
    "wordfreq = sorted(wordfreq.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8inUeYJ2jSER"
   },
   "outputs": [],
   "source": [
    "frequency_flattened = []\n",
    "for list in dic_key:\n",
    "  for item in list:\n",
    "    item = item.strip()\n",
    "    frequency_flattened.append(item)\n",
    "\n",
    "wordfreq = {}\n",
    "for word in frequency_flattened:\n",
    "    if word not in wordfreq:\n",
    "        wordfreq[word] = 0 \n",
    "    wordfreq[word] += 1\n",
    "wordfreq = sorted(wordfreq.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgxKgPQXUk3h",
    "outputId": "6a773eaf-2bf6-4575-faf4-dd9833863f73"
   },
   "outputs": [],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with Metadata pre-screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.MESH = dic_MESH.copy()\n",
    "df_meta_only = pd.get_dummies(df_merged[\"MESH\"].apply(pd.Series).stack()).sum(level=0)\n",
    "df_merged[\"SHORTLIST?\"] = (df_merged[\"SHORTLIST?\"] == 'YES').astype('int')\n",
    "df_meta_only[\"SHORTLIST?\"] = df_merged[\"SHORTLIST?\"].copy()\n",
    "df_meta_only[\"PMID\"] = df_merged[\"PMID\"].copy()\n",
    "df_meta_only[\"DOI\"] = df_merged[\"DOI\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took 'Transcriptome' out of negative list due to poor performance \n",
    "# took DeliveryofHealthCare from positive to negative list \n",
    "negative_list = ['Animals', 'Mice', 'MolecularDockingSimulation', 'GeneExpressionProfiling', 'Rats', 'Proteomics', 'BindingSites', 'Animal', 'Ligands', 'AminoAcidSequence', 'Plant']\n",
    "positive_list = ['DeliveryofHealthCare', 'ElectronicHealthRecords', 'RadiographicImageInterpretation', 'Radiography', 'SeverityofIllnessIndex', 'ClinicalDecision-Making', 'Electrocardiography', 'TreatmentOutcome', 'Mammography', 'Ultrasonography', 'Asthma', 'PulmonaryDisease', 'ChronicObstructive', 'EarlyDetectionofCancer', 'Hospitals', 'AtrialFibrillation', 'Triage', 'HealthCare', 'Hospitalization', 'Anemia', 'ChronicDisease', 'Radiology', 'CoronaryAngiography', 'ParkinsonDisease', 'Dementia', 'RenalInsufficiency', 'CoronaryArteryDisease', 'AcuteCoronarySyndrome', 'Electroencephalography', 'Echocardiography', 'HeartVentricles', 'CriticalCare', 'PulmonaryEmbolism', 'DiagnosticImaging', 'MagneticResonanceAngiography', 'MyocardialInfarction', 'IntensiveCareUnits']\n",
    " \n",
    "df_sanity_check_negative = df_meta_only[df_meta_only[negative_list].any(1)]\n",
    "df_sanity_check_positive = df_meta_only[df_meta_only[positive_list].any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage positive correctly classified: 0.376865671641791\n",
      "percentage negative correctly classified: 0.9774774774774775\n"
     ]
    }
   ],
   "source": [
    "#How good are the lists overall?\n",
    "print(\"percentage positive correctly classified: \" + str((df_sanity_check_positive[\"SHORTLIST?\"].sum()/len(df_sanity_check_positive))) + '\\n' + \"percentage negative correctly classified: \" + str((1- df_sanity_check_negative[\"SHORTLIST?\"].sum()/len(df_sanity_check_negative))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeliveryofHealthCare accurcacy: 0.0%, and in numbers: 0 are positive. Total frequency 'DeliveryofHealthCare' occurs: 14\n",
      "ElectronicHealthRecords accurcacy: 73.0%, and in numbers: 37 are positive. Total frequency 'ElectronicHealthRecords' occurs: 51\n",
      "RadiographicImageInterpretation accurcacy: 70.0%, and in numbers: 16 are positive. Total frequency 'RadiographicImageInterpretation' occurs: 23\n",
      "Radiography accurcacy: 48.0%, and in numbers: 10 are positive. Total frequency 'Radiography' occurs: 21\n",
      "SeverityofIllnessIndex accurcacy: 44.0%, and in numbers: 7 are positive. Total frequency 'SeverityofIllnessIndex' occurs: 16\n",
      "ClinicalDecision-Making accurcacy: 31.0%, and in numbers: 4 are positive. Total frequency 'ClinicalDecision-Making' occurs: 13\n",
      "Electrocardiography accurcacy: 46.0%, and in numbers: 6 are positive. Total frequency 'Electrocardiography' occurs: 13\n",
      "TreatmentOutcome accurcacy: 8.0%, and in numbers: 1 are positive. Total frequency 'TreatmentOutcome' occurs: 13\n",
      "Mammography accurcacy: 57.99999999999999%, and in numbers: 7 are positive. Total frequency 'Mammography' occurs: 12\n",
      "Ultrasonography accurcacy: 40.0%, and in numbers: 4 are positive. Total frequency 'Ultrasonography' occurs: 10\n",
      "Asthma accurcacy: 30.0%, and in numbers: 3 are positive. Total frequency 'Asthma' occurs: 10\n",
      "PulmonaryDisease accurcacy: 30.0%, and in numbers: 3 are positive. Total frequency 'PulmonaryDisease' occurs: 10\n",
      "ChronicObstructive accurcacy: 30.0%, and in numbers: 3 are positive. Total frequency 'ChronicObstructive' occurs: 10\n",
      "EarlyDetectionofCancer accurcacy: 20.0%, and in numbers: 2 are positive. Total frequency 'EarlyDetectionofCancer' occurs: 10\n",
      "Hospitals accurcacy: 50.0%, and in numbers: 4 are positive. Total frequency 'Hospitals' occurs: 8\n",
      "AtrialFibrillation accurcacy: 12.0%, and in numbers: 1 are positive. Total frequency 'AtrialFibrillation' occurs: 8\n",
      "Triage accurcacy: 62.0%, and in numbers: 5 are positive. Total frequency 'Triage' occurs: 8\n",
      "HealthCare accurcacy: 12.0%, and in numbers: 1 are positive. Total frequency 'HealthCare' occurs: 8\n",
      "Hospitalization accurcacy: 71.0%, and in numbers: 5 are positive. Total frequency 'Hospitalization' occurs: 7\n",
      "Anemia accurcacy: 33.0%, and in numbers: 2 are positive. Total frequency 'Anemia' occurs: 6\n",
      "ChronicDisease accurcacy: 14.000000000000002%, and in numbers: 1 are positive. Total frequency 'ChronicDisease' occurs: 7\n",
      "Radiology accurcacy: 28.999999999999996%, and in numbers: 2 are positive. Total frequency 'Radiology' occurs: 7\n",
      "CoronaryAngiography accurcacy: 43.0%, and in numbers: 3 are positive. Total frequency 'CoronaryAngiography' occurs: 7\n",
      "ParkinsonDisease accurcacy: 33.0%, and in numbers: 2 are positive. Total frequency 'ParkinsonDisease' occurs: 6\n",
      "Dementia accurcacy: 33.0%, and in numbers: 2 are positive. Total frequency 'Dementia' occurs: 6\n",
      "RenalInsufficiency accurcacy: 40.0%, and in numbers: 2 are positive. Total frequency 'RenalInsufficiency' occurs: 5\n",
      "CoronaryArteryDisease accurcacy: 50.0%, and in numbers: 3 are positive. Total frequency 'CoronaryArteryDisease' occurs: 6\n",
      "AcuteCoronarySyndrome accurcacy: 60.0%, and in numbers: 3 are positive. Total frequency 'AcuteCoronarySyndrome' occurs: 5\n",
      "Electroencephalography accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'Electroencephalography' occurs: 5\n",
      "Echocardiography accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'Echocardiography' occurs: 5\n",
      "HeartVentricles accurcacy: 60.0%, and in numbers: 3 are positive. Total frequency 'HeartVentricles' occurs: 5\n",
      "CriticalCare accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'CriticalCare' occurs: 5\n",
      "PulmonaryEmbolism accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'PulmonaryEmbolism' occurs: 5\n",
      "DiagnosticImaging accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'DiagnosticImaging' occurs: 5\n",
      "MagneticResonanceAngiography accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'MagneticResonanceAngiography' occurs: 5\n",
      "MyocardialInfarction accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'MyocardialInfarction' occurs: 5\n",
      "IntensiveCareUnits accurcacy: 20.0%, and in numbers: 1 are positive. Total frequency 'IntensiveCareUnits' occurs: 5\n"
     ]
    }
   ],
   "source": [
    "#Worst classifiers\n",
    "for item in positive_list:\n",
    "    df_temp = df_meta_only[df_meta_only[[item]].any(1)]\n",
    "    print(item + ' accurcacy: ' + str(round((df_temp[\"SHORTLIST?\"].sum()) /df_temp[item].count(),2)*100) + '%, and in numbers: ' + str((df_temp[\"SHORTLIST?\"].sum())) + \" are positive. Total frequency '\" + item + \"' occurs: \" + str(df_temp[item].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMIDs_for_prefiltering = df_sanity_check_negative[\"PMID\"].tolist()\n",
    "DOIs_for_prefiltering = df_sanity_check_negative[\"DOI\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbc9NASeo3Gq"
   },
   "source": [
    "# Meta data ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "j1TaA-3xwp5V"
   },
   "outputs": [],
   "source": [
    "df_merged.MESH = dic_MESH.copy()\n",
    "df_meta_only = pd.get_dummies(df_merged[\"MESH\"].apply(pd.Series).stack()).sum(level=0)\n",
    "df_merged[\"SHORTLIST?\"] = (df_merged[\"SHORTLIST?\"] == 'YES').astype('int')\n",
    "df_meta_only[\"SHORTLIST?\"] = df_merged[\"SHORTLIST?\"].copy()\n",
    "\n",
    "#df_meta_only.drop([col for col, val in df_meta_only.sum().iteritems() if val < 6], axis=1, inplace=True)\n",
    "\n",
    "y = df_meta_only[\"SHORTLIST?\"]\n",
    "X = df_meta_only.drop(columns=\"SHORTLIST?\")\n",
    "\n",
    "# train test split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LdJ-w3eypsH",
    "outputId": "c2b5937f-c3b4-42c1-cebd-07b640b0d87d"
   },
   "outputs": [],
   "source": [
    "#sm = SMOTE(random_state=42)\n",
    "#X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "clf = RandomForestClassifier(class_weight = {1:4})\n",
    "rf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5E_TyzlkjZS5",
    "outputId": "584eaac4-b33c-4cd2-b740-e55d8ab96901"
   },
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "best_random.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWHBLsCjY-mr"
   },
   "outputs": [],
   "source": [
    "y_pred = best_random.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbh7Oo3gcjgs"
   },
   "outputs": [],
   "source": [
    "importances = best_random.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in best_random.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "MYj7UE-BdGA6",
    "outputId": "a48a114f-73c6-45a5-ca5b-e1ec17a4a893"
   },
   "outputs": [],
   "source": [
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "ByCah68ndeim",
    "outputId": "69e930a9-cfc3-45e2-afbe-fbcebe7a5445"
   },
   "outputs": [],
   "source": [
    "X.columns[188]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "hbI5tdmMdl4F",
    "outputId": "f35a7fd2-f29f-439e-b01d-b247d6505315"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "y_score = best_random.predict(X_test)\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "disp = plot_precision_recall_curve(best_random, X_test, y_test)\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add aditional Testset for NLP classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Local\n",
    "raw_data_path = 'Data/wo_Meta/unprocessed/validation2.xlsx'\n",
    "destination_folder = 'Data/wo_Meta/preprocessed/additional'\n",
    "\n",
    "# Read raw data\n",
    "df_raw = pd.read_excel(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare columns\n",
    "df_raw['label'] = (df_raw['label'] == 'YES').astype('int')\n",
    "df_raw['titletext'] = df_raw['title'] + \". \" + df_raw['text']\n",
    "\n",
    "# Drop rows with empty text\n",
    "df_raw.drop( df_raw[df_raw.text.str.len() < 5].index, inplace=True)\n",
    "df_raw = df_raw[df_raw['title'].notnull()]\n",
    "#df_raw = df_raw[df_raw['text'].fillna(\"\")]\n",
    "\n",
    "def trim_string(x):\n",
    "\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Trim text and titletext to first_n_words\n",
    "#first_n_words = 1000\n",
    "#df_raw['text'] = df_raw['text'].apply(trim_string)\n",
    "#df_raw['titletext'] = df_raw['titletext'].apply(trim_string) \n",
    "\n",
    "# Drop title and text\n",
    "df_raw.drop([\"title\", \"text\"], axis= 1, inplace=True)\n",
    "\n",
    "# Drop emtpy rows\n",
    "nan_value = float(\"NaN\")\n",
    "df_raw.replace(\"\", nan_value, inplace=True)\n",
    "df_raw.dropna(subset = [\"titletext\"], inplace=True)\n",
    "\n",
    "# Assign X and y\n",
    "X = df_raw[[\"titletext\" ,\"PMID\"]]\n",
    "y = df_raw[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "2     1\n",
       "3     1\n",
       "4     0\n",
       "5     0\n",
       "     ..\n",
       "88    1\n",
       "92    0\n",
       "93    0\n",
       "94    0\n",
       "95    1\n",
       "Name: label, Length: 71, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Write preprocessed data\n",
    "X_train.to_csv(destination_folder + '/X_train.csv', index=False)\n",
    "y_train.to_csv(destination_folder + '/y_train.csv', index=False)\n",
    "X_test.to_csv(destination_folder + '/X_test.csv', index=False)\n",
    "y_test.to_csv(destination_folder + '/y_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_MIT_PubMed.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}