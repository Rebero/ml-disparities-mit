{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall simpletransformers transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jLsgbsgr8Ok",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "import logging \n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "df_author = pd.read_excel(\"./data/training/1229_subset_authorinfo.xlsx\", engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVkPTrlQ7iK0"
   },
   "outputs": [],
   "source": [
    "df_first = df_author[[\"FA AFFILIATION\", \"FA domain\"]]\n",
    "df_last = df_author[[\"LA AFFILIATION\", \"LA domain\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3c4jBzj7jRt",
    "outputId": "a1ca31ab-d49f-4c76-c4c7-5bf446ab5176"
   },
   "outputs": [],
   "source": [
    "df_first = df_first.rename(columns={'FA AFFILIATION': 'AFFILIATION', 'FA domain': 'DOMAIN'})\n",
    "df_last = df_last.rename(columns={'LA AFFILIATION': 'AFFILIATION', 'LA domain': 'DOMAIN'})\n",
    "df_combined = df_first.append(df_last, ignore_index=True)\n",
    "\n",
    "df_combined.loc[df_combined['DOMAIN'] == \"Both?\", 'DOMAIN'] = \"Domain expert\"\n",
    "df_combined.loc[df_combined['DOMAIN'] == \"DOmain expert\", 'DOMAIN'] = \"Domain expert\"\n",
    "df_combined.loc[df_combined['DOMAIN'] == \"Doma+K278\", 'DOMAIN'] = \"Domain expert\"\n",
    "df_combined.loc[df_combined['DOMAIN'] == \".\", 'DOMAIN'] = \"other\"\n",
    "df_combined[\"DOMAIN\"] = df_combined['DOMAIN'].fillna(\"other\")\n",
    "\n",
    "df_combined[\"DOMAIN\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iL0iyC-WBLR3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df_combined[\"AFFILIATION\"]\n",
    "df_combined[\"DOMAIN\"] = df_combined[\"DOMAIN\"].astype(\"category\")\n",
    "df_combined[\"DOMAIN_cat\"] = df_combined[\"DOMAIN\"].cat.codes\n",
    "y = df_combined[\"DOMAIN_cat\"].astype(int)\n",
    "\n",
    "## train valid split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    " \n",
    "df_train = X_train.to_frame().join(y_train)\n",
    "df_valid = X_valid.to_frame().join(y_valid)\n",
    "df_train.columns=[\"text\", \"labels\"]\n",
    "df_valid.columns=[\"text\", \"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "Tp2b8zkcJJjL",
    "outputId": "f855ed40-1e4e-41d8-f67e-f9b39b603880"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    " \n",
    "sweep_config = {\n",
    "    'method': 'random', #grid, random, bayes\n",
    "    'metric': {\n",
    "      'name': 'eval_loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [3e-5, 2e-5]\n",
    "        },\n",
    "        'num_train_epochs':{\n",
    "            'values':[32,64,128,256]\n",
    "        },\n",
    "        'train_batch_size': {\n",
    "            'values':[32,64,128,256]\n",
    "        },\n",
    "        'eval_batch_size': {\n",
    "            'values':[8,16,32]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config,project=\"author_biases_in_ml-0221\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "model_args = ClassificationArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.evaluate_during_training = True\n",
    "#model_args.num_train_epochs = 60\n",
    "#model_args.use_early_stopping = True\n",
    "#model_args.early_stopping_delta = 0.1\n",
    "#model_args.early_stopping_metric = \"mcc\"\n",
    "#model_args.early_stopping_metric_minimize = False\n",
    "#model_args.early_stopping_patience : 5\n",
    "#model_args.manual_seed = 4\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.lazy_loading = False\n",
    "model_args.save_optimizer_and_scheduler = False\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.save_model_every_epoch = False \n",
    "model_args.wandb_project = \"author_biases_in_ml-0221\"\n",
    "\n",
    "\n",
    "def train():\n",
    "    wandb.init()\n",
    "    \n",
    "    model = simpletransformers.classification.ClassificationModel(\"bert\", \"monologg/biobert_v1.1_pubmed\", num_labels=3, args=model_args, use_cuda=True, sweep_config=wandb.config)\n",
    " \n",
    "    # Train the model\n",
    "    model.train_model(df_train, eval_df = df_valid)\n",
    "    #wandb.save('model.h5')\n",
    "    #PATH = wandb.run._run_id + '.h5'\n",
    "    #torch.save(model, PATH)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval_model(df_valid)\n",
    "    #wandb.save('model.h5')\n",
    "    \n",
    "    wandb.join()\n",
    "    torch.cuda.empty_cache()\n",
    "    del model\n",
    " \n",
    "wandb.agent(sweep_id,function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chY-UeX9IH6G"
   },
   "outputs": [],
   "source": [
    "\"\"\"#Alternative\n",
    "def train():\n",
    "    wandb.init()\n",
    "    \n",
    "    # Training arguments\n",
    "    train_args = {\n",
    "        'num_train_epochs' : 30,\n",
    "        'evaluate_during_training': True,\n",
    "        'save_eval_checkpoints': False,\n",
    "        'overwrite_output_dir': True,\n",
    "        'use_multiprocessing' : True,\n",
    "        'reprocess_input_data' : True,\n",
    "        'fp16': False,\n",
    "        'use_early_stopping' : True,\n",
    "        'early_stopping_delta' : 0.1,\n",
    "        'early_stopping_metric' : \"mcc\",\n",
    "        'early_stopping_metric_minimize' : False,\n",
    "        'early_stopping_patience' : 5,\n",
    "        'train_batch_size': wandb.config.train_batch_size,\n",
    "        'eval_batch_size': wandb.config.eval_batch_size,\n",
    "        'learning_rate': wandb.config.learning_rate,\n",
    "        'wandb_project': \"author_biases_in_ml-0221\"\n",
    "    }\n",
    "    \n",
    "    model = simpletransformers.classification.ClassificationModel(\"bert\", \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", num_labels=3, args=train_args, use_cuda=True)\n",
    " \n",
    "    # Train the model\n",
    "    model.train_model(df_train, eval_df = df_train)\n",
    "    #wandb.save('model.h5')\n",
    "    #PATH = wandb.run._run_id + '.h5'\n",
    "    #torch.save(model, PATH)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval_model(df_valid)\n",
    "    #wandb.save('model.h5')\n",
    " \n",
    "wandb.agent(sweep_id,function=train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "author_background_disparities_in_ml.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
